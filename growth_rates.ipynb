{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data import and preprocessing\n",
    "Welcome to growth_rates! There are two cells in this script. The first finds Excel files you have targeted for analysis, imports, and preprocesses data including blank contamination-checking and subtraction. This data is all stored in tidy format (one observation per row) in a pandas DataFrame called 'df_reps'. After making sure the file_target variable is targeting the correct files and that the raw Excel data export is in the '../xlsx/' folder, run the following cell. Once it is finished running, you will see some descriptive statistics and example rows from the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "'''\n",
    "Defining relative location of data to current script and other path variables. \n",
    "Data file must be placed in a folder labeled data in the same folder as this script.\n",
    "'''\n",
    "\n",
    "file_target = input('String to target files? To analyze all files in directory, return without typing any characters.')\n",
    "\n",
    "absolute_path = Path().absolute()\n",
    "data_location = os.path.join(absolute_path, 'xlsx/')\n",
    "if file_target:\n",
    "    results_location = os.path.join(absolute_path, 'results', file_target)\n",
    "else:\n",
    "    results_location = os.path.join(absolute_path, 'results')\n",
    "\n",
    "if not os.path.exists(results_location):\n",
    "    os.makedirs(results_location)\n",
    "\n",
    "print(\"Reading data files from: \", data_location)\n",
    "\n",
    "\n",
    "'''Defining helper methods and functions to be used later'''\n",
    "'''Used to retrieve rows by lookup of a value in another column'''\n",
    "def rowtrieve(self, column, target):\n",
    "    return self.loc[self[column] == target]\n",
    "pd.DataFrame.rowtrieve = rowtrieve\n",
    "\n",
    "'''Generates a list of possible well names'''\n",
    "def char_range(letter1, letter2):\n",
    "    for index in range(ord(letter1), ord(letter2)+1):\n",
    "        yield chr(index)\n",
    "\n",
    "\"\"\"\n",
    "This block generates well_list, which the algorithm can use to differentiate well names from other columns.\n",
    "It includes all default well names on a 96- or 384-well plate.\n",
    "\"\"\"  \n",
    "letters = [letter for letter in char_range('A','P')]\n",
    "numbers = [str(x) for x in range(1,25)]\n",
    "well_list = [str1 + str2 for str1 in letters for str2 in numbers]\n",
    "\n",
    "'''\n",
    "This function rounds small time discrepancies recorded during the gathering of data across the plate\n",
    "in order to standardize analyzed timepoints across wells.\n",
    "'''\n",
    "def HMS_rounder(timestring, unit='min'):\n",
    "    timestring = str(timestring)\n",
    "    h,m,s = timestring.split(':')\n",
    "    h,m,s = (float(val) for val in [h,m,s])\n",
    "   \n",
    "    s_round = round(s/60)\n",
    "    m_round = round((m+s/60)/60)\n",
    "    \n",
    "    if unit == 'min':\n",
    "        return h+(m+s_round)/60\n",
    "    \n",
    "    if unit == 'hour':\n",
    "        return h+m_round\n",
    "\n",
    "def time_parser(string):\n",
    "    \n",
    "    return_value = None\n",
    "    try:\n",
    "        return_value = string.hour + string.minute/60 + string.second/3600\n",
    "    except:\n",
    "        try:\n",
    "            splitstring = string.split('_')[-1]\n",
    "            unit = splitstring.lstrip('0123456789')\n",
    "            value = splitstring[:-len(unit)]\n",
    "\n",
    "            if unit == 'h':\n",
    "                return_value = float(value)\n",
    "            if unit == 'm':\n",
    "                return_value = float(value)/60\n",
    "            if unit == 's':\n",
    "                return_value = float(value)/3600\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return return_value\n",
    "\n",
    "'''\n",
    "Following functions for Excel import functionalities\n",
    "'''\n",
    "\n",
    "'''\n",
    "This function is a lightweight, first-pass function to gather overall file info, export format, \n",
    "and user-given sample variable assignments (from second sheet of Excel doc)\n",
    "'''\n",
    "def xl_info_gather(file, experiment_title):\n",
    "    with open(file):\n",
    "        xlsx = pd.ExcelFile(file)\n",
    "        xl_read = pd.read_excel(xlsx)\n",
    "        export_format = xl_read.iloc[0,3]\n",
    "        \n",
    "        sample_dicts = pd.read_excel(file, sheet_name=1, header=1)\n",
    "    \n",
    "    return xlsx, xl_read, sample_dicts, export_format\n",
    "\n",
    "\n",
    "def kinetic_exporter(xl_read, local_df, sample_dicts):\n",
    "\n",
    "    \"\"\"\n",
    "    Start Excel read comprehension and reorganization into tidy format.\n",
    "    header column contains \"Plate:\" and \"~End\" pointers that demarcate the start and end rows of data,\n",
    "    from which start and end indices can be derived. A file containing multiple row zones delimited by\n",
    "    these pointers should seamlessly find all data. By default, program expects data to begin 2 rows after\n",
    "    \"Plate:' and end 2 rows before \"~End\". \n",
    "    \"\"\"    \n",
    "    header = xl_read.columns[0]\n",
    "    start_index = xl_read.rowtrieve(header, 'Plate:').index\n",
    "    end_index = xl_read.rowtrieve(header, '~End').index\n",
    "\n",
    "    \"\"\"\n",
    "    Tabular data in well-by-well column format e.g. [Time, Temperature, A1, A2, ..., H12] is melted to\n",
    "    reorganize 'Well' into a singular column with measurement 'Value' to be in tidy format\n",
    "    with each data value having its own row.\n",
    "\n",
    "    This block also implements some cleaning and standardizing functions, such as applying the mode Temperature\n",
    "    and time rounding function. Note that if these standardizing functions are turned off, it will negatively \n",
    "    impact the behavior of later groupby functions that rely on certain variables to remain constant across measurements.\n",
    "\n",
    "    Assumptions: \n",
    "    Time in HH:MM:SS format\n",
    "    Assay (e.g., 'Absorbance', 'Fluorescence') 5 cells to the right of 'Plate:'\n",
    "    Static temperature (applying mode Temperature only intended to standardize for <~1C fluctuations)\n",
    "    \"\"\"\n",
    "    idfs = [local_df]\n",
    "    for start, end in zip(start_index,end_index):\n",
    "        # Restructuring Excel formatted data\n",
    "        idf = pd.DataFrame(data = xl_read.iloc[start+2:end-1].values, columns = xl_read.iloc[start+1].values)\n",
    "        sample_vars = [value for value in idf.columns if value not in well_list]\n",
    "        well_vars = [value for value in idf.columns if value in well_list]\n",
    "        idf = pd.melt(idf, id_vars = sample_vars, value_vars = well_vars)\n",
    "\n",
    "        # Cleaning, info-gathering, and standardizing functions\n",
    "        idf = idf.rename(columns={'Temperature(¡C)': 'Temperature', 'variable': \"Well\", 'value': 'Value'})\n",
    "        if 'Temperature' in idf.columns:\n",
    "            idf['Temperature'] = idf['Temperature'].mode().values[0]\n",
    "        idf['Assay'] = xl_read.iat[start, 5] # modify this line if needing a different lookup location for Assay\n",
    "        idf['Time'] = idf['Time'].apply(HMS_rounder)\n",
    "        idf = idf.sort_values(by=['Well', 'Time'])\n",
    "        \n",
    "        # Apply sample conditions to idf and collect it for future concatenation into local_df\n",
    "        idf = idf.merge(sample_dicts, how = 'inner', on = 'Well')\n",
    "        \n",
    "        idfs.append(idf)\n",
    "    \n",
    "    local_df = pd.concat(idfs)\n",
    "        \n",
    "    return local_df\n",
    "\n",
    "\n",
    "'''\n",
    "Wrapper function for possible differences in export format \n",
    "Records each analyzed xlsx file as its own 'Experiment'\n",
    "\n",
    "Includes a toehold for customization, as many different Excel plate export formats exist\n",
    "'''\n",
    "def raw_data_excel_import(file, xlsx_file = True):\n",
    "    \n",
    "    experiment_title = file.split('\\\\')[-1].split('.')[0]\n",
    "    print('Analyzing file {}'.format(experiment_title))\n",
    "\n",
    "    if xlsx_file == True:\n",
    "        xlsx, xl_read, sample_dicts, export_format = xl_info_gather(file, experiment_title)      \n",
    "        local_df = pd.DataFrame()\n",
    "        \n",
    "        if export_format == 'TimeFormat':\n",
    "            local_df = kinetic_exporter(xl_read, local_df, sample_dicts)\n",
    "\n",
    "        elif export_format == 'YourCustomFormat':\n",
    "            pass # Call your custom export formatting function here\n",
    "            # It should take the same inputs and return the same outputs as kinetic_exporter()\n",
    "            # e.g.: local_df = custom_function(xl_read, local_df, sample_dicts)\n",
    "            \n",
    "    local_df['Experiment'] = experiment_title\n",
    "    \n",
    "    return local_df\n",
    "\n",
    "'''\n",
    "This function removes suspected contaminated blanks before averaging for background subtraction.\n",
    "Different media types are evaluated separately.\n",
    "Currently, it uses a statistic of z-score > 2 OD600 value compared to all blanks on the same plate to classify.\n",
    "Also drops a blank with OD600 > 0.5.\n",
    "Statistical method could be improved.\n",
    "\n",
    "Additionally, it currently filters out all rows from df_reps that are not labeled 'Absorbance' under 'Assay'.\n",
    "This behavior is acceptable for a script that strictly evaluates growth rates\n",
    "but may be undesirable for other types of analysis (e.g., fluorescence).\n",
    "'''\n",
    "def data_cleaner(df):\n",
    "    '''Removes wells labeled 'drop' from analysis'''\n",
    "    df = df.loc[df.loc[df['Sample'].str.strip() != 'drop'].index]\n",
    "    cleaned_dfs = []\n",
    "\n",
    "    for keys, media_df in df.groupby(['Media', 'Experiment']):\n",
    "        \n",
    "        blanks_abs_df = media_df.rowtrieve('Sample','blank').rowtrieve('Assay','Absorbance')\n",
    "        blanks_abs_df.loc[:, 'Value'] = pd.to_numeric(blanks_abs_df.Value, errors='coerce').dropna()\n",
    "\n",
    "        \"\"\"\n",
    "        This block finds and removes suspected contaminated blank wells. \n",
    "        It also raises an error if no passable blank wells are found \n",
    "        and warns the user if the number of passable blank wells is 3 or lower.\n",
    "\n",
    "        You can turn this behavior off by switching the find_contamination variable to False.\n",
    "        \"\"\"\n",
    "        find_contamination = True\n",
    "        if find_contamination:\n",
    "            contaminated_blanks = blanks_abs_df[((stats.zscore(blanks_abs_df['Value'].astype(float))) > 2) \n",
    "                | (blanks_abs_df['Value'] > 0.5)]['Well'] # modify the values in this assignment to tune thresholds\n",
    "            if not contaminated_blanks.empty:\n",
    "                print('Blank wells {} of media type {} in experiment {} determined to be contaminated and were removed from analysis.'\n",
    "                      .format(set(contaminated_blanks.values), keys[0], keys[1]))\n",
    "            if len(blanks_abs_df.loc[~blanks_abs_df['Well'].isin(contaminated_blanks.values)]) == 0:\n",
    "                raise ValueError('No passable blank wells found')\n",
    "            elif len(blanks_abs_df.loc[~blanks_abs_df['Well'].isin(contaminated_blanks.values)]) < 4:\n",
    "                warnings.warn('Number of passable blank wells is low')\n",
    "            media_df = media_df.loc[~media_df['Well'].isin(contaminated_blanks.values)]\n",
    "        \n",
    "        '''\n",
    "        Background subtraction: important that blank wells are labeled 'blank' on the excel sheet. \n",
    "        Subtracts different media types separately and performs the calculation independently for each plate. \n",
    "        '''\n",
    "        abs_df = media_df.rowtrieve('Assay', 'Absorbance').reset_index(drop=True)\n",
    "        abs_df['Value'] = abs_df['Value'] - np.mean(abs_df.rowtrieve('Sample', 'blank')['Value'])\n",
    "\n",
    "        cleaned_dfs.append(abs_df)\n",
    "        \n",
    "    df_reps = pd.concat(cleaned_dfs)\n",
    "    \n",
    "    '''Removes blank wells from df after their function to background subtract has been carried out'''\n",
    "    df_reps = df_reps.loc[df_reps['Sample'] != 'blank']\n",
    "\n",
    "    return df_reps\n",
    "\n",
    "\n",
    "df_reps = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "raw_data_excel_import function gathers certain variables from xlsx datasheets and pulls on helper functions\n",
    "to output data into tidy-formatted pandas dataframe, df_reps\n",
    "\n",
    "Following block iterates through relevant data files and aggregates them all into df_reps, with unique ID by 'Experiment' \n",
    "'''\n",
    "for file in sorted(glob.glob(data_location + '*.xls*'), key = lambda x: int(re.sub('\\\\D', '', x))):\n",
    "    if '.xlsx' in file and file_target in file:\n",
    "        df_reps = pd.concat([df_reps, raw_data_excel_import(file)], ignore_index=True) \n",
    "    '''\n",
    "    I tried to get pandas to read xls files directly but some computer jank occurred \n",
    "    that I don't have time to solve so it's best to just convert files to xlsx.\n",
    "    '''\n",
    "    if '.xls' in file and '.xlsx' not in file:\n",
    "        print('Hello, please do a simple export of your .xls raw data files to .xlsx')\n",
    "        print('Here\\'s the file you need to convert: {}'.format(file))\n",
    "\n",
    "df_reps = data_cleaner(df_reps)\n",
    "print('\\n Here are some descriptive statistics of df_reps:')\n",
    "print(df_reps.describe(include='all'))\n",
    "\n",
    "print('\\n Here are some example rows from df_reps:')\n",
    "print(df_reps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best fit calculation and export\n",
    "The second cell is where best fits are calculated. Optimal fit parameters are gathered for each well in 'df_fits' and these parameters are aggregated by mean and standard devation for each sample into 'df_means'. These pandas DataFrames are then exported into separate Excel files under '../results/' to allow graphing and visualization however you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "'''Defining key lists for later use'''\n",
    "parameter_names = ['Growth rate (/hr)','logT Y-intercept','R2','XTimeStart','XTimeStop']\n",
    "constant_cols = df_reps.columns.difference(['Time','Value'])\n",
    "\n",
    "'''Timer decorator'''\n",
    "def timer(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t0 = time.perf_counter()\n",
    "        value = func(*args, **kwargs)\n",
    "        t1 = time.perf_counter()\n",
    "        print('Function {} took {:.4f} seconds to complete'.format(func.__name__, t1-t0))\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "def well_fitter(df_kinetic):\n",
    "    '''\n",
    "    This code tests \"windows\" of integers numbers of successive timepoints (window_size) for their fit \n",
    "    to a linear regression of time (x_data) v. log-transformed OD600 (y_data). The function does the log transform itself,\n",
    "    so y_data should be given in terms of raw OD600 values.\n",
    "\n",
    "    The input x_data and y_data are for one well. \n",
    "    Extrapolating this function to the entire plate is the job of the wrapper function.\n",
    "\n",
    "    I force the fit to work over an OD600 window that corresponds to typical exponential phase by:\n",
    "    1) forcing an e^1.5 (~4.5)-fold increase between yStart and yStop values (filtering out non-exponential phase fits)\n",
    "    2) forcing a minimum value of e^-3 (~0.05) as the yStop value (filtering out fits from noise at low absorbances)\n",
    "    3) forcing a fit R^2 value of >0.99 in order to classify as a valid fit to filter out fitting to subpar data\n",
    "    4) forcing a minimum fit window of 100 minutes (i.e., 10 timepoints spaced at 10 minutes each)\n",
    "    \n",
    "    These values may need to be shifted depending on the setup.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def find_linear_regime(x_data, y_data, window_size):\n",
    "        \"\"\"\n",
    "        This function tests a single window size over the entire range of data \n",
    "        and returns the best linear regression fit for that window as determined by R value.\n",
    "        \n",
    "        Growth data should be supplied to this function in their raw (non-transformed) form.\n",
    "        \"\"\"\n",
    "        def fast_batch_linregress(X_windows, Y_windows):\n",
    "            \"\"\" Perform linear regression on all windows simultaneously \n",
    "            using numpy sliding windows for fast computation. \"\"\"\n",
    "            n = X_windows.shape[1]\n",
    "\n",
    "            '''Compute sums across each window'''\n",
    "            sum_x = X_windows.sum(axis=1)\n",
    "            sum_y = Y_windows.sum(axis=1)\n",
    "            sum_x2 = (X_windows**2).sum(axis=1)\n",
    "            sum_y2 = (Y_windows**2).sum(axis=1)\n",
    "            sum_xy = (X_windows * Y_windows).sum(axis=1)\n",
    "\n",
    "            '''Calculate slope and intercept for each window'''\n",
    "            slopes = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n",
    "            intercepts = (sum_y - slopes * sum_x) / n\n",
    "\n",
    "            '''Calculate R² values'''\n",
    "            r_numerator = n * sum_xy - sum_x * sum_y\n",
    "            r_denominator = np.sqrt((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))\n",
    "            r_values = r_numerator / r_denominator\n",
    "            r_squared = r_values**2\n",
    "\n",
    "            return slopes, intercepts, r_squared\n",
    "        \n",
    "        x_windows = np.lib.stride_tricks.sliding_window_view(x_data, window_size)\n",
    "        y_windows = np.lib.stride_tricks.sliding_window_view(y_data, window_size)\n",
    "        \n",
    "        '''Run batch linear regression on all windows'''\n",
    "        slopes, intercepts, r_squared = fast_batch_linregress(x_windows, y_windows)\n",
    "         \n",
    "        '''Apply filtering criteria to all windows simultaneously'''\n",
    "        valid_growth = (y_windows[:, -1] - y_windows[:, 0] > 1.5) & (y_windows[:, -1] > -3)\n",
    "        valid_fits = r_squared > 0.99\n",
    "        valid_windows = valid_growth & valid_fits\n",
    "        \n",
    "        \n",
    "        '''Find the best fit by R²'''\n",
    "        if np.any(valid_windows):\n",
    "            best_idx = np.argmax(r_squared[valid_windows])\n",
    "            best_window_fit = [slopes[valid_windows][best_idx], intercepts[valid_windows][best_idx], r_squared[valid_windows][best_idx],\n",
    "                    x_windows[valid_windows][best_idx][0], x_windows[valid_windows][best_idx][-1],]\n",
    "            return best_window_fit\n",
    "        \n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    '''Deriving measurement interval from time data'''\n",
    "    total_culture_time = int(math.floor(df_kinetic['Time'].max()-df_kinetic['Time'].min()))\n",
    "    interval = total_culture_time/(df_kinetic['Time'].count()-1)\n",
    "\n",
    "    '''\n",
    "    Setting possible window sizes (i.e., number of timepoints analyzed together) from time parameters.\n",
    "    min_window has lower bounds of 100 minutes and 8 timepoints.\n",
    "    max_window had upper bounds of 100 hours and all timepoints.\n",
    "    '''\n",
    "    min_window = max(math.floor(100/60/interval), 8)\n",
    "    max_window = min(math.floor(100/interval), df_kinetic['Time'].count())\n",
    "\n",
    "    '''Defining variables that will be passed for analysis'''\n",
    "    X = df_kinetic['Time'].astype('float').reset_index(drop=True)\n",
    "    Y = df_kinetic['Value'].astype('float').reset_index(drop=True)\n",
    "    logY = np.log(Y)\n",
    "    \n",
    "    '''Iterating through and finding best fits for all allowed window sizes'''\n",
    "    well_fits = []\n",
    "    for window_size in range(min_window,max_window,1):\n",
    "        window_fit = find_linear_regime(X, logY, window_size)\n",
    "        if window_fit:\n",
    "            well_fits.append(window_fit)\n",
    "\n",
    "    '''Retaining descriptive variables and restructuring for eventual merge into df_fits'''\n",
    "    descriptors = df_kinetic[constant_cols].drop_duplicates().T.squeeze(axis=1)\n",
    "    expt_well = descriptors[['Experiment','Well']]\n",
    "    other_index = descriptors.index.difference(['Experiment','Well'])\n",
    "    other_descriptors = descriptors[other_index]\n",
    "\n",
    "    '''\n",
    "    If-else to insulate from cases where all windows failed to provide an acceptable fit,\n",
    "    wherein fit parameters will be added to df_fits as NaN and sample descriptors will be maintained.\n",
    "\n",
    "    If acceptable fits exist, the fit with the highest R^2 value is identified for addition to df_fits.\n",
    "    '''\n",
    "    if well_fits:\n",
    "        well_array = np.array(well_fits)\n",
    "        best_idx = np.argmax(well_array[:,2])\n",
    "        best_fit = pd.Series(well_array[best_idx], name = descriptors.name, index = parameter_names)\n",
    "        \n",
    "    else:\n",
    "        best_fit = pd.Series(name = descriptors.name, index = parameter_names)\n",
    "\n",
    "    best_fit = pd.concat([expt_well, best_fit, other_descriptors], axis=0)\n",
    "        \n",
    "    return best_fit\n",
    "    \n",
    "'''\n",
    "This function iterates through each well of each plate of each experiment \n",
    "and uses the well_fitter function on it.\n",
    "'''\n",
    "@timer\n",
    "def plate_absorbance_fitter(df):\n",
    "    df_abs = df.rowtrieve('Assay','Absorbance')\n",
    "    fits_list = []\n",
    "    for keys,group in df_abs.groupby(['Well', 'Experiment']):\n",
    "        fit = well_fitter(group)\n",
    "        if not fit.empty:\n",
    "            fits_list.append(fit)\n",
    "\n",
    "    '''Build df_fits then sort by (1) experiment, then (2) well order.'''\n",
    "    df_fits = pd.DataFrame(fits_list)     \n",
    "    df_fits['Well'] = pd.Categorical(df_fits['Well'], ordered=True, categories = well_list)\n",
    "    df_fits.sort_values(by=['Experiment','Well'], inplace=True)\n",
    "    \n",
    "    cols = df_fits.columns.tolist()\n",
    "\n",
    "    return df_fits\n",
    "\n",
    "df_fits = plate_absorbance_fitter(df_reps)\n",
    "\n",
    "'''This block derives df_means, which provides mean and standard deviation across replicates of fit parameters, from df_fits'''\n",
    "'''Defining aggregation functions'''\n",
    "agg_dict = dict.fromkeys(parameter_names,['mean','std'])\n",
    "agg_dict['Well'] = lambda x: x.tolist()\n",
    "\n",
    "'''Building df_means, a DataFrame that aggregates df_fits'''\n",
    "replicate_parameters = [par for par in constant_cols if par != 'Well']\n",
    "df_means = df_fits.groupby(replicate_parameters).agg(agg_dict).rename(columns={'Well':'Wells'})\n",
    "df_means['Replicates with valid fits'] = df_fits.groupby(replicate_parameters)[parameter_names[0]].count()\n",
    "df_means['Average Doubling Time (hrs)'] = np.log(2)/df_means.loc[:,(parameter_names[0],'mean')]\n",
    "\n",
    "'''Export fit results to an Excel file in the results location'''\n",
    "if not file_target:\n",
    "    df_fits.to_excel(results_location + '/Best fits by well (all).xlsx', index=False)\n",
    "    df_means.to_excel(results_location + '/Mean fits by grouped replicates (all).xlsx')\n",
    "else:\n",
    "    df_fits.to_excel(results_location + '/Best fits by well.xlsx', index=False)\n",
    "    df_means.to_excel(results_location + '/Mean fits by grouped replicates.xlsx')\n",
    "print('Find individual fit and replicate-grouped parameters in {}'.format(results_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gif_maker",
   "language": "python",
   "name": "gif_maker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "panel-cell-order": [
   "8cd9d10a",
   "5a510650",
   "03958bc8",
   "bf7dc8d5"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
